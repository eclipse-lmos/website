"use strict";(self.webpackChunklmos_website=self.webpackChunklmos_website||[]).push([[9224],{6286:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/AgentDevelopment","metadata":{"permalink":"/lmos/blog/AgentDevelopment","source":"@site/blog/AgentDevelopment.md","title":"Agent Development","description":".","date":"2025-01-17T00:00:00.000Z","tags":[{"inline":true,"label":"AI","permalink":"/lmos/blog/tags/ai"},{"inline":true,"label":"LMOS","permalink":"/lmos/blog/tags/lmos"},{"inline":true,"label":"Agents","permalink":"/lmos/blog/tags/agents"}],"readingTime":16.04,"hasTruncateMarker":false,"authors":[{"name":"Rahul Jamwal","key":null,"page":null}],"frontMatter":{"title":"Agent Development","description":".","date":"2025-01-17T00:00:00.000Z","authors":[{"name":"Rahul Jamwal"}],"tags":["AI","LMOS","Agents"]},"unlisted":false,"nextItem":{"title":"Spring.ai Integration","permalink":"/lmos/blog/Spring.AI-integration"}},"content":"In a world increasingly powered by AI, creating reliable and efficient agents isn\u2019t just about technology but also about processes, strucutures and methodologies. This blog will take you behind the scenes of our agent development, and the various phases involved until production rollout.\\n\\nThe development of an AI agent, much like a traditional development, begins with a requirements elicitation and documentation phase. However, the agent development has many different and additional phases which we briefly will touch upon in this article. A simplified diagrammatic view of agent development might look something as this:\\n\\n![Alt text](/img/agent-development.png)\\n\\nTo begin with, we first look at the production dataset available for conversations and service tickets created by our human agents.\\n\xa0\\nIn most CRM systems, there is a broad classification already available underneath which few set of conversation details are logged by human agents.\\nFor example, there might a conversation with the customer related to a 5G network issue, this may be logged in the CRM system under Network issues category which may have further subcategories.\xa0\\n\\nWe looked at this information and determined the categories for which the largest number of tickets were logged in a month. For example, Customers may be calling mostly for Billing related queries. Using these statistics available from production, it is determined which Business domain should be prioritized first.\\nAn extensive analysis of this CRM data is done to determine the different category of problems a customer seeks help for under the Billing domain. For each category it is determined what set of knowledge will be required to answer or help with those queries or problems. In some cases the knowledge is right away available from the CRM tickets but in other cases, we may have to scrap data off from webpages, pdf documents or other available knowledge APIs.\\n\\nAs such, the outcome of the Requirements phase is that we usually end up with a detailed story, which is then further broken down into multiple stories for different Teams:\\n\\n- Knowledge Operations Team\\n- AI Agent Engineering Team\\n- Data Science Team\\n- Security and Compliance Team\\n\\n**Knowledge Operations Team** is usually tasked with building pipelines to scrap data off sources such as help and support pages etc, and integrating these pipelines with the vector search services to convert this data into embeddings representations stored in vector databases.\\n\\n**Agent Engineering Team** is tasked with development of AI agents that orchestrate and interact with other agents, models, and services in order to cater to the queries for the domain to which that agent belongs.\\n\\n**Agent Data Science Team** takes care of determining what is the best model to be used for the specific use cases and language. Our Data Science team also writes initial prompts and runs them through several iterations to get the desired results.\\n\\n**Security and Compliance Team** helps out with providing guidelines and doing assessments to ensure we stay compliant to all the GDPR and other security, privacy, and ethical guidelines.\\n\\nHere is an example story , our teams usually create sub-tasks or sub-stories under each story for different teams.\\n\\n> Example story:\\n> \\n> As a Billing agent, I need use all relevant knowledge & prompts at my disposal to appropriately answer one of the following Billing query.\\n>\\n>\\n> **Agent inscope capabilities:**\\n> The billing agent must be able to answer anything related to the following:\\n> - Breakdown of bill - Explain  and elucidate any question related to tax calculation,\xa0\xa0extra      data usage charges, recurring bill amounts.\\n> - Bill amount surge - In cases  when backend provides supporting data, the agent must be able to answer  why my current bill is higher than last month\'s bill.\\n> - Compare bills - Be able to compare bills for two months and spot the differences leading to a higher      bill amount.\\n> - Link to down bills - ability  to fetch and share urls for last three month bills.\\n> - Charge codes on bill - be able to explain special charge codes on the bills in details for example \'RCMG-1mon\' may suggest an international charge code on the bill for one month.\\n> - Third party references - be able to refer the customer to third party portals when there is a third      party charge on the bill.\\n> - Bill cycles - be able to  guide the customer how bill cycle can be changed if needed.\\n> - Payment status - be able to answer which bills have been fully paid and which have an outstanding amount.\\n>\\n> **Agent out of scope capabilities:** \\n> - Following would be consider out of scope for the capabilities of the agent:\\n> - Anything that is classified as non-billing question by the intent classifier.\\n> - Anything in the Billing intent but which requires access to specific data that is currently not available.\\n> - Anything related to helping the customer make an instant payment using the agent.\\n>\\n> **KPIs:** The agent\'s success criteria would be coverage of 90% of the in-scope queries with a resolution rate of 80%, 5% fabricating information, and 2% risky information.\\n>\\n> **Decision framework:**\xa0Production agent conversation data was analyzed. A classification was made on the high level support type tickets, and it was determined that 50% of the support queries are related to Billing.\\n> With this agent going live and meeting the defined KPIs, we expect to deflect approximately 5000 daily calls off the call center.\\n>\\n> **Utterances:**\\n> Sample utterances as noted from our agent conversation database, is attached for references and for tests and validations.\xa0\xa0This data has been refined by our Team of data scientists to make it usable for tests and validations.\\n> \\n> **Test Data:**\\n> - Account existing in the billing system with a bill having consecutive surge over last two months - 2342938498\\n> - Account with only one paid bill in the last three months - 342342342\\n> - Account with special international roaming charge and OTT subscription charge - 9273293482\\n>\\n> **Samples best response examples as per Utterances dataset:**\\n>\\n> Use this data as an example for LLMs, prompt refinements and/or multi-shot trainings.\\n>\\n> Sample Conversation1\\n>\\n> Sample Conversation2\\n>\\n> Sample Conversation3\\n\\n### Development Phase\\n\\nIn the development phase our teams undertake several tasks. LMOS platform minimizes the developers efforts and ensures most of the recurring steps and complex interactions are taken care of so developer can only focus on the key enhancements related to the Business domain.\\n\\nHere are some examples:\\n\\n#### Step functions:\\nSeveral common functions that have to be undertaken by an agent have been modeled as a step, for example, if there is a url that has been generated by the LLM then it is ascertained in this step whether this is a whitelisted url that can be passed back to the customer or not (removing hallucinated urls). We maintain an easily configurable list of urls that can be updated through a simple CMS frontend by anyone.\\n\\nAnother example might be to classify & rephrase the complete previous LLM conversation so it can be passed as a contextual information in the next request to the LLM.\\n\\nIn general the development of most agents require some pre-defined mandatory steps which are take care of by the platform, for example logging final response, but some steps are configurable and can be skipped depending on capabilities needed from the agent.\\n\\nThere might be many existing steps which are provided by the platform, however depending on the capabilities needed in the agent and Business use cases, new steps may have to be introduced by our Teams. It is carefully ascertained if something can be incorporated as a step in core platform or if it can be added as a validation code in the agent. Our team of architects and software designers take appropriate decisions to guide our team members as needed.\\n\\nFor a case when dynamic control is needed, flow of prompts are modified using pre-defined, configurable templates.\\n\\n#### Flow of Prompts:\\nIn several cases, the execution flow for an agent may require some dynamic modifications in run time or may need tweaks via prompt engineering without any code interventions by the Engineering teams.\\n\\nThese would be the classic cases such as depending on the input language, region, tenant or channel we may have to execute some specific code, for example it might be an IVR channel and an additional step might be needed to fine tune the tone or conciseness of the response.\\nThis flow of prompts is usually defined via prompt templates, and we have prompt templates created for every agent.\\n\xa0\\n#### Retrieved Augmented Generation:\xa0\\nFor every new agent, accessing RAG knowledge is all about writing a single line of code, and specifying the tenant since LMOS platform natively supports multi-tenancy and decides, on the basis of tenant input, as to which RAG database to query for embeddings.\\n\\nHowever, with each new agent, we have to supplement the RAG with new knowledge of the domain. Some of this knowledge is available from pdfs, some from help and support pages and some from pre-existing anonymized conversations.\\n\\nIn each case, we have dedicated knowledge pipelines which is integrated with our search services and these knowledge sources are connected to these pipelines to extract and transform the data into embedding representations stored in our vector databases. This is a task that is undertaken by our Knowledge Operations Team that scours the available resources for the best source of knowledge and\xa0integrate them into our pipelines.\\nRAG is a big topic and you can read more about it at [RAG overview](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview). \\n\\n\\n#### Backend APIs:\xa0\\nThere are several cases where specific information pertaining to a user\'s usage is required, as such an access to some backend APIs is required.\\nIn our system, we have a carefully guarded backend database accessible only via a secure API layer after appropriate authentication & authorizations through a central gateway. Access is strictly controlled on a need to basis and limited only to required non-private datasets.\\n\\nBased on the Business use cases, several new APIs may be required to be invoked.\\nThe developers write corresponding **LLM functions** for every API that caters to a specific business case, these functions on the basis of Business domains are grouped together with clear annotated descriptions and information for each of these APIs is passed to the LLM to understand which functions is needed to invoke a specific API for fetching specific details.\\n\\nEach such **LLM function** also goes through privacy and security review to ensure we stay compliant to norms and regulations, and entity recognition models (Anonymization models) are used to determine and scrub SPI data.\\n\\nLMOS platform takes care of the underlying complexities such as:\\n- Transforming the functions to appropriate API request models as per LLM API specifications.\\n- Invocation of APIs as requested by the LLM via functions.\\n- Transforming the response back into a an LLM API format.\\n- Handling the LLM response and handing over to the agent for further processing.\\n\\n#### Prompting - Dynamic, natural language based:\\nOur ARC DSL allows anyone from the team to define prompts in a simple natural language. The prompt engineer has an arsenal of dynamic functions which can be used in prompts to enhance the prompt in real time with additional information which can be fetched from the system during an actual flow. For example, you might want to send a specific url to the LLM depending on the type of customer asking the query, if it is a Business customer there might be a greeting or formal tone to be used. The separation between ARC module for prompting and platform code module allows faster prompt iterations, easy modifications and quicker deployments.\\n\\nIn many cases depending on the Business requirements our teams may require to write new functions which can be used in the dynamic prompt flows.\\n\\nSince prompts also support multi-tenancy hence there can be a different prompt which is configurable per channel and tenant for the same step, for example for prompt flow step of \'ClassifyAnswer\' you can have a different prompt for IVR and different prompt for chat conversations.\\n\\nDuring prompt engineering, our Agent Data Science team iteratively tweaks & validates the prompts so that the BOT response reaches a specific quality threshold.\\n\\nIn addition during prompt engineering, the Data Science members may also take several decision regarding whether to add additional knowledge into the knowledge database or supplement some information within prompts.\\n\\nThe supplemented knowledge may be about some general information that a BOT is supposed to know in terms of the guidelines of an organizations about communications with the customers, or some procedures followed to do something on behalf of the customer. In such cases, usually pdfs or other sources are procured, data is parsed processed, refined into a format and it can either be added as an information in the prompts or it can be added in the RAG by passing via embedding models.\\n\\nIn some cases, it is ascertained that some dynamic information is needed during runtime on the basis of which the LLM is supposed to take appropriate decisions. For example, In some cases it might be helpful for the LLM to know how many services the customer currently has so as to be able to answer better.\\n\\nIn other cases, it might be noticed that disambiguation is needed and the LLMs are instructed to ask specific number of queries in order to better understand the customer\'s query or to help with a better resolution.\\n\\n### Validation Phase\\n\\n#### Annotations:\\nResponses generated by an LLM powered system are not always accurate, factually correct and reliable. One should ensure to integrate a human in the loop approach for any system that is powered by LLMs before rolling out major functionalities/changes into production specifically for applications where high stakes are involved.\\n\\nChatbot applications may need human feedbacks in many different cases/scenarios:\\n- Evaluate the system before rolling out a new feature in production.\\n- Recurring regression evaluations to uncover any unknown issues.\\n- Evaluate a new model, maybe for anonymization or embedding.\\n- Knowledge retrieval and ranking.\\n\\nFor the LMOS platform, for any new feature or Business domain that is introduced it is always ensured that human evaluators are sharing their feedbacks in a preprod environment before that feature goes live. Our framework has an integrated step \'Response Evaluation\' through which another LLM is used to determine the quality and appropriateness of response before passing it on to the customer, however for any new major developments our team always ensures to have human evaluators evaluate the system typically on edge cases that may not conform to typical patterns.\\n\\nConversations or queries are fed into the LMOS agents via a chat frontend, and the complete conversational record turn by turn is saved into a database. We currently make use of Opensearch, and ensure each conversation and every message in the conversation can be uniquely identified with a conversationId, turnId respectively.\\n\\nThese conversations or queries are obtained from a processed dataset of conversational records from our CRM systems. In case you have a chat based system (LLM based or guided decision tree based chat system) you can also use these chat conversations, refine & process them into a format that can be fed into this system to generate responses for evaluations.\xa0\\n\\nThese generated conversations are basically scrapped via a script and refined into a specific dataset format which can be accepted by a specific tool used for evaluation. For evaluation of our LMOS agentic system, we make use of Argilla which allows us to define and adapt the evaluation criteria and questions as per our requirements. Argilla also supports displaying UI elements like tile, card etc. in case your chat application makes use of those so that the human evaluators are aware of how information was presented to the customers.\\n\\nThe human evaluators are supposed to rate every response on certain parameters for example:\\n- Does this response contain any sensitive information ?\xa0\\n- How appropriate is this response on a scale of 1-5 ?\\n\\nAfter the end of evaluation, the evaluations by human are represented usually in terms of some of these key metrics:\\n- Risky %\\n- Fabricating Info %\\n- Response Quality %\\n- Hallucination %\\n\\nThe values for these KPIs are generated by Argilla on the basis of defined weightage for each parameter or question.\\n\\nSpecific quality threshold are defined which have to be met for each of those KPIs before any new feature or agent is brought live into production. In case the generated numbers are below the acceptable value threshold then the system is handed back to the Engineers and Data scientists with appropriate feedbacks. Our team of Engineers and Data scientists, and Knowledge Ops members work together to ensure that appropriate optimizations are put into place to bring the system to an acceptable quality level, this can include prompt optimizations, RAG optimizations,\xa0etc.\\n\\nOnce the new feature of agent meets an acceptable quality level, then the agent is put into the next phase of Business validation.\\n\\nIn addition, recurring annotations are also done in production to evaluate our system on a weekly basis on the same quality KPIs.\\n\\n#### Business Validations:\\nSince LMOS provides out of the box capabilities such as Blue Green and A/B, we are able to quickly deploy a new agent to a parallel beta test env on production infrastructure.\\n\\nOur team of Business domain experts assess the quality of response of the bot and provide their feedbacks. Based on these feedbacks the dev teams take appropriate actions to correct the prompts and/or supplement the bot with additional knowledge as needed.\\n\\nOn reaching an appropriate quality threshold the agents are made live for real customers.\\n\\n### Monitoring and Improvements\\n\\nPost go live, our teams continuously monitor the performance of the BOT. Continuous evaluation tools are used to keep an eye on the performance of the BOT.  Frequent manual quality assessments are also done using annotation tools like Argilla.\\n\\nAppropriate feedbacks noted from production monitoring & quality validations are noted in the backlog as improvement tasks, which are further prioritized and taken up into the sprints.\\n\\n#### Differences - Agents vs Traditional development\\n\\nIt is certainly evident to our Development Teams that the outcomes from different iterations of tests are quite unpredictable due to the nature of the LLM models. Our teams make their best attempts at reducing this degree of uncertainty by engaging in various tweaks to the prompts, sharing examples to the BOT, controlling temperature etc. parameters.\\n\\nThis also brings with it the unpredictability in the estimates on the completion of such work which gets some of us thinking that some kind of unpredictability constant must be factored in while calculating the story point estimates for some of these stories.\\n\\nIn addition, there are no established patterns of the kind that are usually found in traditional domains or applications centered around domain driven designs. This leads to our Teams doing more of experimentation, learning from it and then moving forward with data driven decision making. \\n\\nThere are many ethical considerations which are also kept at the back of mind while designing such applications that leverage LLMs."},{"id":"Spring.AI-integration","metadata":{"permalink":"/lmos/blog/Spring.AI-integration","source":"@site/blog/SpringAiClient.md","title":"Spring.ai Integration","description":"As the world of AI continues to evolve and develop,","date":"2024-07-10T00:00:00.000Z","tags":[{"inline":true,"label":"spring","permalink":"/lmos/blog/tags/spring"},{"inline":true,"label":"Spring.AI","permalink":"/lmos/blog/tags/spring-ai"},{"inline":true,"label":"feature","permalink":"/lmos/blog/tags/feature"}],"readingTime":2.335,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"Spring.AI-integration","title":"Spring.ai Integration","date":"2024-07-10T00:00:00.000Z","tags":["spring","Spring.AI","feature"]},"unlisted":false,"prevItem":{"title":"Agent Development","permalink":"/lmos/blog/AgentDevelopment"},"nextItem":{"title":"Llama3 is out!","permalink":"/lmos/blog/Llama3"}},"content":"As the world of AI continues to evolve and develop,\\nthe need of integration of AI services into your applications \\nhas become increasingly valuable.\\n\\nSince ARC\'s goal is to make the process of integration, management \\nand creation of AI in your existing services as seamless as possible \\nit only made sense to integrate Spring.AI to work in a plug and play \\nfashion.\\n \\n\x3c!-- truncate --\x3e\\n\\nIn this blog post, we\'ll explore how to implement a new adapter for \\nSpring.AI to work within ARC.\\nTo expand its capabilities and make it easier to incorporate various \\nAI services into your projects.\\n\\n## What is an the `SpringChatClient` Adapter in ARC?\\n\\nIn ARC, the `SpringChatClient` acts like a bridge between the framework and \\nexternal Spring.AI models/ APIs. It enables you to seamlessly integrate any \\nSpring.AI ChatModels in your ARC application, allowing for easy access to their \\nfunctionality. These Adapters/ ChatClients can be used to connect to various \\nAI platforms, such as Google VertexAI, Amazon Comprehend, Grog, Mistral.Ai, \\nIBM Watson or many [more](https://docs.spring.io/spring-ai/reference/api/chatmodel.html).\\n\\n## Why use the New Adapter?\\n\\nThe Adapter allows for quick ruse of existing AI model API\'s written by Spring.AI.\\nThis will allow any developer familiar with Spring.AI to get a head start to further\\nget to know and love the unique features that come with ARC.\\n\\n\\nHowever there are some *limitations* to the use of Spring.AI models in ARC.\\nSince they are not written with re-loadability and DSL in mind not all \\nkey features will work.\\n\\n\\n## Step-by-Step Guide to Implementing a New Adapter\\n\\nTo implement a new adapter for Spring.AI, follow these steps:\\n\\n1. **Choose an AI Service:** Select the AI model you\'d like to integrate\\nwith your application. In this case we will re-implement the natively \\nexisting [ollama client](blog/Llama3.md).\\n\\n1. **Get it done:** Since we try to eliminate boilerplate this wont take \\nlong :).\\n\\n```kotlin title=\'chatCompleterProvider for Ollama\'\\npackage io.github.lmos.arc.Spring.AI\\n\\n// Reusing Spring.AI models and terminologies\\nimport org.springframework.ai.ollama.OllamaChatModel\\nimport org.springframework.ai.ollama.api.OllamaApi\\nimport org.springframework.ai.ollama.api.OllamaOptions\\nimport org.springframework.boot.autoconfigure.SpringBootApplication\\nimport org.springframework.context.annotation.Bean\\n\\n@SpringBootApplication\\nopen class YourApplication {\\n\\n    @Bean\\n    open fun chatCompleterProvider(ollamaApi: OllamaApi) = SpringChatClient(\\n        OllamaChatModel(\\n            OllamaApi(\\"http://localhost:8888\\"),\\n            OllamaOptions.create().withModel(\\"llama3:8b\\")),\\n            \\"llama3:8b\\",\\n    )\\n\\n}\\n```\\n## Conclusion\\n\\nUsing the new adapter for Spring.AI `SpringChatClient` can greatly reduce\\nthe time of implementation for anyone who has used Spring.AI before. \\nBy following the before mentioned steps, you can integrate any of the \\nSpring.AI models that cater to the specific needs of your applications.\\n\\nRemember to choose an AI service that aligns with your project\'s goals and\\nrequirements, and don\'t hesitate to reach out if you have any questions or\\nneed further guidance on implementing a ARC agent. Happy coding!\\n\\n:::tip full-potential\\nBe sure to create a custom implementation of the `ChatCompleter` interface or \\nany of the many predefined chatCompleters within the ARC repo to unleash the\\nfull potential of the nimble ARC framework for agent creation.\\n:::"},{"id":"/Llama3","metadata":{"permalink":"/lmos/blog/Llama3","source":"@site/blog/Llama3.md","title":"Llama3 is out!","description":"Meta have released a new version of their large language model, Llama3, https://ai.meta.com/blog/meta-llama-3/.","date":"2024-04-25T00:00:00.000Z","tags":[],"readingTime":0.365,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Llama3 is out!","date":"2024-04-25T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Spring.ai Integration","permalink":"/lmos/blog/Spring.AI-integration"}},"content":"Meta have released a new version of their large language model, Llama3, https://ai.meta.com/blog/meta-llama-3/.\\n\\nThanks to ollama, the model can easily be tested locally, see https://ollama.com/library/llama3\\n\\n\x3c!-- truncate --\x3e\\n\\nSimply pull the model, run `ollama` and start using it with Arc!\\n\\n```bash\\nollama pull llama3:8b\\nollama serve\\n```\\n\\nNow llama3:8b is ready to use with Arc and the ollama client.\\nWe recommend using the 8B variant locally, as it \\"only\\" requires 8GB of RAM."}]}}')}}]);